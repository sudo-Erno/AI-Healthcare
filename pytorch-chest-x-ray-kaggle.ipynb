{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image","metadata":{"id":"gvUt9wtE87e4","execution":{"iopub.status.busy":"2022-02-28T01:42:00.617503Z","iopub.execute_input":"2022-02-28T01:42:00.617801Z","iopub.status.idle":"2022-02-28T01:42:02.364959Z","shell.execute_reply.started":"2022-02-28T01:42:00.617718Z","shell.execute_reply":"2022-02-28T01:42:02.364219Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-02-28T01:42:02.368421Z","iopub.execute_input":"2022-02-28T01:42:02.368618Z","iopub.status.idle":"2022-02-28T01:42:11.296092Z","shell.execute_reply.started":"2022-02-28T01:42:02.368594Z","shell.execute_reply":"2022-02-28T01:42:11.295293Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Declaring the paths of training, validation and test data","metadata":{"id":"ak3rwKjE5S_M"}},{"cell_type":"code","source":"# Train\ntrain_images = []\ntrain_labels = []\nnormal_train_paths = r\"../input/chest-xray-pneumonia/chest_xray/train/NORMAL/\"\npneumonia_train_paths = r\"../input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/\"\n\nfor img_name in os.listdir(normal_train_paths):\n  train_images.append(os.path.join(normal_train_paths, img_name))\n  train_labels.append(0)\n\nfor img_name in os.listdir(pneumonia_train_paths):\n  train_images.append(os.path.join(pneumonia_train_paths, img_name))\n  train_labels.append(1)\n\nprint(len(normal_train_paths), len(pneumonia_train_paths))\n\n# Val\nval_images = []\nval_labels = []\nnormal_val_paths = r\"../input/chest-xray-pneumonia/chest_xray/val/NORMAL/\"\npneumonia_val_paths = r\"../input/chest-xray-pneumonia/chest_xray/val/PNEUMONIA/\"\n\nfor img_name in os.listdir(normal_val_paths):\n    val_images.append(os.path.join(normal_val_paths, img_name))\n    val_labels.append(0)\n\nfor img_name in os.listdir(pneumonia_val_paths):\n    val_images.append(os.path.join(pneumonia_val_paths, img_name))\n    val_labels.append(1)\n\nprint(len(normal_val_paths), len(pneumonia_val_paths))\n\n# Test\ntest_images = []\ntest_labels = []\nnormal_test_paths = r\"../input/chest-xray-pneumonia/chest_xray/test/NORMAL/\"\npneumonia_test_paths = r\"../input/chest-xray-pneumonia/chest_xray/test/PNEUMONIA/\"\n\nfor img_name in os.listdir(normal_test_paths):\n    test_images.append(os.path.join(normal_test_paths, img_name))\n    test_labels.append(0)\n\nfor img_name in os.listdir(pneumonia_test_paths):\n    test_images.append(os.path.join(pneumonia_test_paths, img_name))\n    test_labels.append(1)\n\nprint(len(normal_test_paths), len(pneumonia_test_paths))","metadata":{"id":"OHx1ueCL9KYq","outputId":"63eaf45e-a2cc-43be-f898-92c39be7c8d5","execution":{"iopub.status.busy":"2022-02-28T01:42:11.297514Z","iopub.execute_input":"2022-02-28T01:42:11.297761Z","iopub.status.idle":"2022-02-28T01:42:11.743655Z","shell.execute_reply.started":"2022-02-28T01:42:11.297730Z","shell.execute_reply":"2022-02-28T01:42:11.742909Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(len(train_images))\nprint(len(val_images))\nprint(len(test_images))","metadata":{"id":"WtFPEW2HuzTq","outputId":"654598a9-e539-4739-8670-a1ce6a556e04","execution":{"iopub.status.busy":"2022-02-28T01:42:11.744962Z","iopub.execute_input":"2022-02-28T01:42:11.745206Z","iopub.status.idle":"2022-02-28T01:42:11.751772Z","shell.execute_reply.started":"2022-02-28T01:42:11.745172Z","shell.execute_reply":"2022-02-28T01:42:11.750899Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Choosing device","metadata":{"id":"HjH3Hm346DKr"}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"7kl-0AREFWGt","outputId":"27d21b4d-effb-4743-bb55-d748494cbdbc","execution":{"iopub.status.busy":"2022-02-28T01:42:11.755574Z","iopub.execute_input":"2022-02-28T01:42:11.756853Z","iopub.status.idle":"2022-02-28T01:42:11.804795Z","shell.execute_reply.started":"2022-02-28T01:42:11.756821Z","shell.execute_reply":"2022-02-28T01:42:11.804069Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Create a transformation pipeline\n- Read the images as PIL and convert them into tensors.\n- Normalize them.","metadata":{"id":"GWZXQQu9vy4h"}},{"cell_type":"code","source":"from torchvision.transforms.transforms import ColorJitter\n\nnew_size = 224\nbatch_size = 16\n\nnormalize = transforms.Normalize(mean=[0.5], std=[0.5])\n\ntrain_transform = transforms.Compose([\n                                transforms.ToPILImage(),\n                                transforms.Resize((new_size, new_size)),\n                                transforms.ColorJitter(brightness=(0.95, 1.05), contrast=(0.95, 1.05), saturation=(0.95, 1.05)),\n                                transforms.ToTensor()\n])\n\nval_transform = transforms.Compose([\n                                transforms.ToPILImage(),\n                                transforms.Resize((new_size, new_size)),\n                                transforms.ToTensor()\n])","metadata":{"id":"l_kHAZodwDRe","execution":{"iopub.status.busy":"2022-02-28T01:42:11.805943Z","iopub.execute_input":"2022-02-28T01:42:11.806331Z","iopub.status.idle":"2022-02-28T01:42:11.814966Z","shell.execute_reply.started":"2022-02-28T01:42:11.806295Z","shell.execute_reply":"2022-02-28T01:42:11.814280Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ChestXRayDataset(Dataset):\n    def __init__(self, images_paths, labels, transforms=None):\n        super().__init__()\n        self.images_paths = images_paths # path of all files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.images_paths)\n\n    def __getitem__(self, ix):\n        img_path = self.images_paths[ix]\n        cls = self.labels[ix]\n\n        img = cv2.imread(img_path)[:,:,::-1]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        \n        cls = torch.tensor([cls]).float().to(device)\n\n        return img, cls\n\n    def collate_fn(self, batch):\n        images, classes = [], []\n    \n        for ix in range(len(batch)):\n            imgs, clss = batch[ix]\n            \n            if self.transforms:\n                imgs = self.transforms(imgs)[None]\n            else:\n                imgs = transforms.ToTensor(imgs)[None]\n\n            imgs = [normalize(i/255.0) for i in imgs]\n\n            images.extend(imgs)\n\n            classes.extend(clss)\n\n        images_tensor = torch.cat(images)\n        images_tensor = images_tensor.view(batch_size, 1, new_size, new_size)\n\n        classes = torch.tensor(classes)\n        classes = classes.view((batch_size, 1))\n\n        return images_tensor.to(device), classes.to(device)","metadata":{"id":"FmuRzFIO-38f","execution":{"iopub.status.busy":"2022-02-28T01:42:11.817841Z","iopub.execute_input":"2022-02-28T01:42:11.818449Z","iopub.status.idle":"2022-02-28T01:42:11.829349Z","shell.execute_reply.started":"2022-02-28T01:42:11.818424Z","shell.execute_reply":"2022-02-28T01:42:11.828646Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# tr_size = int(len(train_images) * 0.5)\n\n# train_ds = ChestXRayDataset(train_images[:tr_size], train_labels[:tr_size], train_transform)\n# val_ds = ChestXRayDataset(val_images, val_labels, val_transform)\n# test_ds = ChestXRayDataset(test_images, test_labels, val_transform)\n\ntrain_ds = ChestXRayDataset(train_images, train_labels, train_transform)\nval_ds = ChestXRayDataset(val_images, val_labels, val_transform)\ntest_ds = ChestXRayDataset(test_images, test_labels, val_transform)\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, collate_fn=train_ds.collate_fn, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=val_ds.collate_fn, shuffle=True)\ntest_dl = DataLoader(test_ds, batch_size=batch_size, collate_fn=test_ds.collate_fn, shuffle=True)","metadata":{"id":"O8M0Ot8kdFOw","execution":{"iopub.status.busy":"2022-02-28T01:42:11.830429Z","iopub.execute_input":"2022-02-28T01:42:11.831076Z","iopub.status.idle":"2022-02-28T01:42:11.841901Z","shell.execute_reply.started":"2022-02-28T01:42:11.831042Z","shell.execute_reply":"2022-02-28T01:42:11.841272Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Checking the labels shape","metadata":{}},{"cell_type":"code","source":"class ChestXModel(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n    self.vgg_backbone = models.vgg16(pretrained=True)\n    self.vgg_backbone.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n    # Removes the inplace operation\n    for name, layer in self.vgg_backbone.named_modules():\n      if isinstance(layer, torch.nn.ReLU):\n        layer.inplace = False\n\n    for param in self.vgg_backbone.parameters():\n      param.requires_grad = False\n\n    features = 25088\n\n    self.vgg_backbone.classifier = nn.Sequential(\n        nn.Linear(features, 128, bias=True),\n        nn.ReLU(),\n        nn.Dropout(p=0.3, inplace=False),\n        nn.Linear(128, 64, bias=True),\n        nn.ReLU(),\n        nn.Dropout(p=0.3, inplace=False),\n        nn.Linear(64, 32, bias=True),\n        nn.ReLU(),\n        nn.Dropout(p=0.3, inplace=False),\n        nn.Linear(32, 1, bias=True),\n        nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    return self.vgg_backbone(x)","metadata":{"id":"egsr113LfI24","execution":{"iopub.status.busy":"2022-02-28T01:42:11.843173Z","iopub.execute_input":"2022-02-28T01:42:11.843603Z","iopub.status.idle":"2022-02-28T01:42:11.854565Z","shell.execute_reply.started":"2022-02-28T01:42:11.843568Z","shell.execute_reply":"2022-02-28T01:42:11.853697Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# TODO: See how to calculate accuracy https://github.com/PacktPublishing/Modern-Computer-Vision-with-PyTorch/blob/master/Chapter06/Road_sign_detection.ipynb\ndef train_batch(data, model, optimizer, criterion):\n\n  imgs, clss = data\n\n  model.train().to(device)\n\n  preds = model(imgs)\n  loss = criterion(preds, clss)\n\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n  return loss.cpu().detach().numpy()\n  # return loss.item()","metadata":{"id":"wy-Cj-ivj7is","execution":{"iopub.status.busy":"2022-02-28T01:42:11.855692Z","iopub.execute_input":"2022-02-28T01:42:11.856081Z","iopub.status.idle":"2022-02-28T01:42:11.863563Z","shell.execute_reply.started":"2022-02-28T01:42:11.856048Z","shell.execute_reply":"2022-02-28T01:42:11.862738Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef val_batch(data, model, criterion):\n    imgs, clss = data\n\n    model.eval().to(device)\n\n    with torch.no_grad():\n\n        preds = model(imgs)\n\n        loss = criterion(preds, clss)\n\n        return loss.cpu().detach().numpy()","metadata":{"id":"T5SdkogPkmZw","execution":{"iopub.status.busy":"2022-02-28T01:42:11.864864Z","iopub.execute_input":"2022-02-28T01:42:11.865156Z","iopub.status.idle":"2022-02-28T01:42:11.874624Z","shell.execute_reply.started":"2022-02-28T01:42:11.865124Z","shell.execute_reply":"2022-02-28T01:42:11.873892Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef accuracy(data, model):\n    x, y = data\n\n    model.eval().to(device)\n\n    with torch.no_grad():\n        \n        preds = model(x)\n\n        acc = (preds > 0.5) == y\n        acc = acc.float().sum()\n        acc = acc / len(y)\n        acc = acc * 100\n\n        return acc.cpu()","metadata":{"id":"5pCxhMWn585m","execution":{"iopub.status.busy":"2022-02-28T01:42:11.875837Z","iopub.execute_input":"2022-02-28T01:42:11.876296Z","iopub.status.idle":"2022-02-28T01:42:11.883340Z","shell.execute_reply.started":"2022-02-28T01:42:11.876220Z","shell.execute_reply":"2022-02-28T01:42:11.882636Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"nj83ZbBxPtmS","execution":{"iopub.status.busy":"2022-02-28T01:42:11.884612Z","iopub.execute_input":"2022-02-28T01:42:11.885843Z","iopub.status.idle":"2022-02-28T01:42:11.891424Z","shell.execute_reply.started":"2022-02-28T01:42:11.885807Z","shell.execute_reply":"2022-02-28T01:42:11.890708Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)","metadata":{"id":"X8Xp3BLgIFJQ","outputId":"d0e78da1-7506-454f-a5cd-da55e40e66ea","execution":{"iopub.status.busy":"2022-02-28T01:42:11.894469Z","iopub.execute_input":"2022-02-28T01:42:11.894894Z","iopub.status.idle":"2022-02-28T01:42:11.900638Z","shell.execute_reply.started":"2022-02-28T01:42:11.894860Z","shell.execute_reply":"2022-02-28T01:42:11.899845Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!mkdir saved_models","metadata":{"execution":{"iopub.status.busy":"2022-02-28T01:42:11.901718Z","iopub.execute_input":"2022-02-28T01:42:11.902349Z","iopub.status.idle":"2022-02-28T01:42:12.568422Z","shell.execute_reply.started":"2022-02-28T01:42:11.902312Z","shell.execute_reply":"2022-02-28T01:42:12.566686Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"n_epochs = 20\nmodel = ChestXModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nloss_per_batch, acc_per_batch = [], []\nval_loss_per_batch, val_acc_per_batch = [], []\n\nfor ex in range(n_epochs):\n    torch.cuda.empty_cache()\n    print(f\"Epoch {ex+1}\")\n  \n    # Training Loss and Accuracy\n    train_loss, train_acc = [], []\n  \n    for i, data in enumerate(train_dl):\n        loss = train_batch(data, model, optimizer, criterion)    \n        acc = accuracy(data, model)\n        \n        train_loss.append(loss)\n        train_acc.append(acc)\n\n    loss_per_batch.append(np.array(train_loss).mean())\n    acc_per_batch.append(np.asarray(train_acc).mean())\n    \n    # Save model\n    torch.save(model.state_dict(), f\"./saved_models/model_epoch{ex}.pt\")\n\n    # Validation Loss and Accuracy\n    val_loss, val_acc = [], []\n    \n    for i, data in enumerate(val_dl):\n        loss = val_batch(data, model, criterion)\n        acc = accuracy(data, model)\n        \n        val_acc.append(acc)\n        val_loss.append(loss)\n\n    val_loss_per_batch.append(np.asarray(val_loss).mean())\n    val_acc_per_batch.append(np.asarray(val_acc).mean())","metadata":{"id":"4CytuCfWk4NX","outputId":"8bed2b8d-a113-438f-f3d3-28737c5d04fc","execution":{"iopub.status.busy":"2022-02-28T01:42:12.571592Z","iopub.execute_input":"2022-02-28T01:42:12.572089Z","iopub.status.idle":"2022-02-28T02:37:20.524572Z","shell.execute_reply.started":"2022-02-28T01:42:12.572048Z","shell.execute_reply":"2022-02-28T02:37:20.523760Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(9, 9), sharex=True)\n\nax1.plot(list(range(n_epochs)), loss_per_batch, label=\"Loss\")\nax1.set_title('Training loss per batch')\nax1.set(xlabel=\"Epochs\", ylabel=\"Loss\")\nax1.grid()\nax1.legend()\n\nax2.plot(list(range(n_epochs)), acc_per_batch, label=\"Acc\")\nax2.set_title('Training accuracy per batch')\nax2.set(xlabel=\"Epochs\", ylabel=\"Accuracy\")\nax2.grid()\nax2.legend()\n\nax3.plot(list(range(n_epochs)), val_loss_per_batch, label=\"Loss\")\nax3.set_title(\"Validation loss per batch\")\nax3.set(xlabel=\"Epochs\", ylabel=\"Loss\")\nax3.grid()\nax3.legend()\n\nax4.plot(list(range(n_epochs)), val_acc_per_batch, label=\"Acc\")\nax4.set_title(\"Validation accuracy per batch\")\nax4.set(xlabel=\"Epochs\", ylabel=\"Accuracy\")\nax4.grid()\nax4.legend()\n\nplt.show()","metadata":{"id":"wsAswurbSIam","execution":{"iopub.status.busy":"2022-02-28T02:37:20.525779Z","iopub.execute_input":"2022-02-28T02:37:20.526033Z","iopub.status.idle":"2022-02-28T02:37:21.073119Z","shell.execute_reply.started":"2022-02-28T02:37:20.526000Z","shell.execute_reply":"2022-02-28T02:37:21.072447Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"CcHSPprx8haZ"}},{"cell_type":"code","source":"test_acc = []\nfor i, data in enumerate(test_dl):\n  acc = accuracy(data, model)\n  test_acc.append(acc)","metadata":{"id":"QJYB8eiN8Kbk","execution":{"iopub.status.busy":"2022-02-28T02:37:21.074224Z","iopub.execute_input":"2022-02-28T02:37:21.075396Z","iopub.status.idle":"2022-02-28T02:37:43.066029Z","shell.execute_reply.started":"2022-02-28T02:37:21.075355Z","shell.execute_reply":"2022-02-28T02:37:43.065289Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_acc = torch.tensor(test_acc).cpu().detach().numpy().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T02:37:43.067496Z","iopub.execute_input":"2022-02-28T02:37:43.067742Z","iopub.status.idle":"2022-02-28T02:37:43.073427Z","shell.execute_reply.started":"2022-02-28T02:37:43.067709Z","shell.execute_reply":"2022-02-28T02:37:43.072766Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_acc)\nplt.rcParams[\"figure.figsize\"] = (9, 9)\nplt.title('Testing accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.grid()\n\nplt.show()","metadata":{"id":"cUjybApJ9Akp","execution":{"iopub.status.busy":"2022-02-28T02:37:43.074781Z","iopub.execute_input":"2022-02-28T02:37:43.075049Z","iopub.status.idle":"2022-02-28T02:37:43.379741Z","shell.execute_reply.started":"2022-02-28T02:37:43.075016Z","shell.execute_reply":"2022-02-28T02:37:43.379084Z"},"trusted":true},"execution_count":20,"outputs":[]}]}